{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn5s51YtHcFS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from matplotlib import pyplot\n",
        "from gensim.models import KeyedVectors\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.35\n",
        "data = pd.read_csv('/content/drive/My Drive/book_summaries_genres.csv', skiprows=lambda i: i>0 and random.random() > p)"
      ],
      "metadata": {
        "id": "fViSlV7sHeT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['genres_string'] = \"\"\n",
        "for index, row in data.iterrows():\n",
        "  if(type(row['genres']) == str):\n",
        "    genre_dict = json.loads(row['genres'])\n",
        "    genre_string = \"\"\n",
        "    for key in genre_dict.keys():\n",
        "\n",
        "      genre_string+=genre_dict[key].replace(\" \", \"\") + \" \"\n",
        "    data.at[index, [\"genres_string\"]] = genre_string"
      ],
      "metadata": {
        "id": "hhf5BV2rHiqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utitlity functions for removing ASCII characters, converting lower case, removing stop words, html and punctuation from description\n",
        "\n",
        "def _removeNonAscii(s):\n",
        "    return \"\".join(i for i in s if  ord(i)<128)\n",
        "\n",
        "def make_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    text = tokenizer.tokenize(text)\n",
        "    text = \" \".join(text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "szr5Dj8zHn31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data.iterrows():\n",
        "  if(type(row['author']) == str):\n",
        "    data.at[index, [\"author\"]] = row['author'].replace(\" \", \"\")\n",
        "  else:\n",
        "    data.at[index, [\"author\"]] = \"\""
      ],
      "metadata": {
        "id": "JWna-LvMHqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['genres_cleaned'] = data['genres_string'].apply(_removeNonAscii)\n",
        "data['genres_cleaned'] = data.genres_cleaned.apply(func = make_lower_case)\n",
        "data['genres_cleaned'] = data.genres_cleaned.apply(func = remove_stop_words)\n",
        "data['genres_cleaned'] = data.genres_cleaned.apply(func=remove_punctuation)\n",
        "data['genres_cleaned'] = data.genres_cleaned.apply(func=remove_html)\n",
        "data['author_cleaned'] = data['author'].apply(_removeNonAscii)\n",
        "data['author_cleaned'] = data.author_cleaned.apply(func = make_lower_case)\n",
        "data['author_cleaned'] = data.author_cleaned.apply(func = remove_stop_words)\n",
        "data['author_cleaned'] = data.author_cleaned.apply(func=remove_punctuation)\n",
        "data['author_cleaned'] = data.author_cleaned.apply(func=remove_html)\n",
        "data['title_cleaned'] = data['title'].apply(_removeNonAscii)\n",
        "data['title_cleaned'] = data.title_cleaned.apply(func = make_lower_case)\n",
        "data['title_cleaned'] = data.title_cleaned.apply(func = remove_stop_words)\n",
        "data['title_cleaned'] = data.title_cleaned.apply(func=remove_punctuation)\n",
        "data['title_cleaned'] = data.title_cleaned.apply(func=remove_html)\n",
        "data['cleaned'] = data['summaries'].apply(_removeNonAscii)\n",
        "data['cleaned'] = data.cleaned.apply(func = make_lower_case)\n",
        "data['cleaned'] = data.cleaned.apply(func = remove_stop_words)\n",
        "data['cleaned'] = data.cleaned.apply(func=remove_punctuation)\n",
        "data['cleaned'] = data.cleaned.apply(func=remove_html)"
      ],
      "metadata": {
        "id": "mti65S8ZHuIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['combined_features'] = data.apply(lambda x: x['author_cleaned'] + ' ' + x['genres_cleaned'], axis=1)"
      ],
      "metadata": {
        "id": "Nj_sSYhPH1S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(analyzer='word',ngram_range=(1, 1),min_df=0, stop_words='english')\n",
        "count_matrix = count.fit_transform(data['combined_features'])"
      ],
      "metadata": {
        "id": "oTmxGCN-H35F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities = cosine_similarity(count_matrix, count_matrix)"
      ],
      "metadata": {
        "id": "Us_jGx_5H9F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = pd.Series(data.index, index=data['title'])\n",
        "titles = data['title']"
      ],
      "metadata": {
        "id": "XHy67UZyH_um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations_additional_feats(title, n=5):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "    return list(titles.iloc[book_indices].values)[:n]"
      ],
      "metadata": {
        "id": "wuElhuXmIE-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_recommendations_additional_feats(data['title'][0]))"
      ],
      "metadata": {
        "id": "gDlVGSsO_EHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for words in data['cleaned']:\n",
        "    corpus.append(words.split())"
      ],
      "metadata": {
        "id": "aaFkRoH4IHbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_FILE = '/content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz'\n",
        "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "# Training our corpus with Google Pretrained Model\n",
        "\n",
        "google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
        "google_model.build_vocab(corpus)\n",
        "\n",
        "#model.intersect_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', lockf=1.0, binary=True)\n",
        "\n",
        "google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
        "\n",
        "google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)"
      ],
      "metadata": {
        "id": "MFtMdLFWIXvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the average word2vec for the each book description\n",
        "\n",
        "def vectors(x):\n",
        "    \n",
        "    # Creating a list for storing the vectors (description into vectors)\n",
        "    global word_embeddings\n",
        "    word_embeddings = []\n",
        "\n",
        "    # Reading the each book description \n",
        "    for line in data['cleaned']:\n",
        "        avgword2vec = None\n",
        "        count = 0\n",
        "        for word in line.split():\n",
        "            if word in google_model.wv.vocab:\n",
        "                count += 1\n",
        "                if avgword2vec is None:\n",
        "                    avgword2vec = google_model[word]\n",
        "                else:\n",
        "                    avgword2vec = avgword2vec + google_model[word]\n",
        "                \n",
        "        if avgword2vec is not None:\n",
        "            avgword2vec = avgword2vec / count\n",
        "        \n",
        "            word_embeddings.append(avgword2vec)"
      ],
      "metadata": {
        "id": "Q6jLVnMTIajM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors(data)\n",
        "cosine_similarities_word2vec = cosine_similarity(word_embeddings, word_embeddings)"
      ],
      "metadata": {
        "id": "N1jzwwA1Iedf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommending the Top 5 similar books\n",
        "\n",
        "def recommendations_word2vec(title):\n",
        "    \n",
        "    # Calling the function vectors\n",
        "\n",
        "    # taking the title and book image link and store in new data frame called books\n",
        "    # books = data[['title', 'image_link']]\n",
        "    books = data[['title']]\n",
        "    #Reverse mapping of the index\n",
        "    indices = pd.Series(data.index, index = data['title']).drop_duplicates()\n",
        "         \n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_similarities_word2vec[idx]))\n",
        "\n",
        "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
        "    sim_scores = sim_scores[1:6]\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "    recommend = books.iloc[book_indices]\n",
        "    for index, row in recommend.iterrows():\n",
        "      print(row)\n",
        "        # response = requests.get(row['image_link'])\n",
        "        # img = Image.open(BytesIO(response.content))\n",
        "        # plt.figure()\n",
        "        # plt.imshow(img)\n",
        "        # plt.title(row['title']) "
      ],
      "metadata": {
        "id": "hZfooiJGIjRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations_word2vec(data['title'][0])"
      ],
      "metadata": {
        "id": "OHZso6cw_NLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_average_count = (2.5*cosine_similarities + 7.5*cosine_similarities_word2vec)/10"
      ],
      "metadata": {
        "id": "UESy2pWEIkGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building TFIDF model and calculate TFIDF score\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df = 5, stop_words='english')\n",
        "tfidf.fit(data['cleaned'])\n",
        "\n",
        "# Getting the words from the TF-IDF model\n",
        "\n",
        "tfidf_list = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\n",
        "tfidf_feature = tfidf.get_feature_names() # tfidf words/col-names"
      ],
      "metadata": {
        "id": "_qdpwb3QJCkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building TF-IDF Word2Vec \n",
        "\n",
        "# Storing the TFIDF Word2Vec embeddings\n",
        "tfidf_vectors = []; \n",
        "line = 0;\n",
        "# for each book description\n",
        "summary_count = 0\n",
        "for desc in corpus:\n",
        "    if(summary_count >= 1000 and summary_count%1000 == 0):\n",
        "      print(summary_count)\n",
        "    summary_count+=1 \n",
        "  # Word vectors are of zero length (Used 300 dimensions)\n",
        "    sent_vec = np.zeros(300) \n",
        "    # num of words with a valid vector in the book description\n",
        "    weight_sum =0; \n",
        "    # for each word in the book description\n",
        "    for word in desc:  \n",
        "        if word in google_model.wv.vocab and word in tfidf_feature:\n",
        "            vec = google_model.wv[word]\n",
        "            tf_idf = tfidf_list[word] * (desc.count(word) / len(desc))\n",
        "            sent_vec += (vec * tf_idf)\n",
        "            weight_sum += tf_idf\n",
        "            # break\n",
        "    if weight_sum != 0:\n",
        "        sent_vec /= weight_sum\n",
        "    tfidf_vectors.append(sent_vec)\n",
        "    line += 1"
      ],
      "metadata": {
        "id": "5Y5rOBsSJFYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities_tf = cosine_similarity(tfidf_vectors,  tfidf_vectors)"
      ],
      "metadata": {
        "id": "eCpN2vu_Jhbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recommending top 5 similar books\n",
        "\n",
        "def recommendations_tfidf(title):\n",
        "    \n",
        "    # finding cosine similarity for the vectors\n",
        "    \n",
        "    # taking the title and book image link and store in new data frame called books\n",
        "    # books = data[['title', 'image_link']]\n",
        "    books = data[['title']]\n",
        "    #Reverse mapping of the index\n",
        "    indices = pd.Series(data.index, index = data['title']).drop_duplicates()\n",
        "  \n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_similarities_tf[idx]))\n",
        "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
        "    sim_scores = sim_scores[1:6]\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "    recommend = books.iloc[book_indices]\n",
        "    for index, row in recommend.iterrows():\n",
        "      print(row['title'])\n",
        "        # response = requests.get(row['image_link'])\n",
        "        # img = Image.open(BytesIO(response.content))\n",
        "        # plt.figure()\n",
        "        # plt.imshow(img)\n",
        "        # plt.title(row['title'])"
      ],
      "metadata": {
        "id": "Nf3GB6KvJHmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations_tfidf(data['title'][0])"
      ],
      "metadata": {
        "id": "SdJ5OAMQ_Tuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_tfidf_count = (2*cosine_sim + 8*cosine_similarities_tf)/10"
      ],
      "metadata": {
        "id": "h9lU-cSVJJyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recommending top 5 similar books\n",
        "\n",
        "def recommendations_tfidf_tf(title):\n",
        "    \n",
        "    # finding cosine similarity for the vectors\n",
        "    \n",
        "    # taking the title and book image link and store in new data frame called books\n",
        "    # books = data[['title', 'image_link']]\n",
        "    books = data[['title']]\n",
        "    #Reverse mapping of the index\n",
        "    indices = pd.Series(data.index, index = data['title']).drop_duplicates()\n",
        "  \n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_tfidf_count[idx]))\n",
        "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
        "    sim_scores = sim_scores[1:6]\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "    recommend = books.iloc[book_indices]\n",
        "    for index, row in recommend.iterrows():\n",
        "      print(row['title'])\n",
        "        # response = requests.get(row['image_link'])\n",
        "        # img = Image.open(BytesIO(response.content))\n",
        "        # plt.figure()\n",
        "        # plt.imshow(img)\n",
        "        # plt.title(row['title'])"
      ],
      "metadata": {
        "id": "QI3ykz9jJfnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations_tfidf_tf(data['title'][0])"
      ],
      "metadata": {
        "id": "DTjE6r7J_ac5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}